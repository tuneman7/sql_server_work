{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62292812-f394-447c-beaf-8b7d71d15a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from libraries.utility import Utility as mutil\n",
    "from libraries.db_base import db_base\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b405a0-a341-4fc9-bd1e-65f0e48a8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = mutil()\n",
    "# Start the timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b31a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 18 for SQL Server' : file not found (0) (SQLDriverConnect)\")\n",
      "Database : products, Connection Good: False\n"
     ]
    }
   ],
   "source": [
    "dbprod = db_base(\"products\")\n",
    "get_product_info_sql=dbprod.get_sql_query_from_query_key(\"get_product_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a1c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"(\" + get_product_info_sql + \") AS query_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c5c7c5-2d32-4683-b8b4-83cb17b15407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/01 05:40:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/01 05:40:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/01 05:40:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|        product_name|        product_type|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|William's Amazing...|Download         ...|\n",
      "|  2|  Hannah's Wild Ride|Bundle           ...|\n",
      "|  3|Amanda's Crazy Co...|Subscription     ...|\n",
      "|  4|Christine's Monst...|Movie            ...|\n",
      "|  5|Dana's Date in th...|Game             ...|\n",
      "|  6|Brandon's Crazy C...|Event            ...|\n",
      "|  7| Joan's Amazing Race|Pay TV           ...|\n",
      "|  8|Lisa's Date in th...|Pay TV           ...|\n",
      "|  9|Anthony's Patriot...|Pay TV           ...|\n",
      "| 10|  Justin's Wild Ride|Subscription     ...|\n",
      "| 11|Sherri's Big Romance|Subscription     ...|\n",
      "| 12|Antonio's Date in...|Bundle           ...|\n",
      "| 13|Amanda's Spy Thri...|Pay TV           ...|\n",
      "| 14| William's Great War|Game             ...|\n",
      "| 15| Michael's Wild Ride|TV Episode       ...|\n",
      "| 16|Margaret's Crazy ...|Pay TV           ...|\n",
      "| 17|Peter's Date With...|Single View Movie...|\n",
      "| 18|Dana's Monster Ma...|Download         ...|\n",
      "| 19|Penny's Patriotic...|Game             ...|\n",
      "| 20|   Helen's Wild Ride|TV Episode       ...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 0 minutes and 3 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def split_file_and_save_parts():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MSSQLConnectionExample\") \\\n",
    "        .config(\"spark.jars\", \"mssql-jdbc-12.4.2.jre11.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:sqlserver://mssql1:1433;databaseName=products\"\n",
    "    database_properties = {\n",
    "        \"user\": \"sa\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "\n",
    "    \n",
    "    # Read data from MSSQL\n",
    "    #df = spark.read.jdbc(url=database_url, table=\"products\", properties=database_properties)\n",
    "\n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=query, properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "split_file_and_save_parts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dced2979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 05:40:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/01 05:40:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+----------+-------------------+-------+----------------+---------------------+---------------------+----------+--------------------+----------+----------+\n",
      "| id|product_id|customer_id|account_id|          post_date|amt_usd|geo_geography_id|fin_distro_channel_id|fin_distro_partner_id|created_by|          created_dt|updated_by|updated_dt|\n",
      "+---+----------+-----------+----------+-------------------+-------+----------------+---------------------+---------------------+----------+--------------------+----------+----------+\n",
      "|  1|       399|          1|       278|2023-08-22 16:09:05| $24.66|           35059|                   26|                  375|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "|  2|       303|          1|       166|2023-08-15 02:36:04| $12.04|           35059|                   22|                  201|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "|  3|       224|          1|       751|2023-08-27 19:29:25| $13.29|           35059|                   11|                  632|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "|  4|        79|          2|       360|2023-03-29 04:34:23| $32.00|           35059|                   23|                  187|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "|  5|       137|          2|       729|2023-01-01 04:40:49| $21.15|           35059|                    6|                   85|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "|  6|       114|          3|       579|2023-03-16 00:51:53|  $6.08|           35056|                    8|                  645|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "|  7|       284|          3|        32|2023-11-25 20:35:09| $19.19|           35056|                   16|                  382|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "|  8|       174|          3|       143|2023-08-11 11:48:12| $25.59|           35056|                    8|                  692|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "|  9|       385|          4|       680|2023-04-09 22:21:29| $22.99|           35054|                   23|                  713|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 10|        49|          4|        91|2023-02-05 12:04:56| $34.18|           35054|                   13|                  144|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 11|       326|          5|       711|2023-01-25 03:51:51| $15.89|           35050|                   16|                  129|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 12|       402|          5|       232|2023-03-08 18:11:29| $34.48|           35050|                   13|                  502|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 13|       395|          6|       137|2023-01-23 17:21:37| $13.46|           35049|                   29|                  381|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 14|       491|          6|       310|2023-06-23 08:29:16| $13.77|           35049|                    2|                  202|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 15|       343|          6|       299|2023-05-07 13:36:16| $14.91|           35049|                   21|                   82|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 16|       482|          7|       556|2023-11-09 00:39:21| $28.82|           35045|                    8|                  329|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 17|       170|          7|       526|2023-01-21 00:32:46| $30.73|           35045|                   18|                  162|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 18|       266|          7|       751|2023-08-31 22:48:17| $24.98|           35045|                   31|                  167|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 19|       119|          8|       555|2023-02-21 12:07:43| $12.81|           35045|                    5|                  510|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "| 20|       104|          8|        15|2023-07-12 06:22:19| $32.58|           35045|                   27|                   75|      NULL|2023-12-01 04:47:...|      NULL|      NULL|\n",
      "+---+----------+-----------+----------+-------------------+-------+----------------+---------------------+---------------------+----------+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 0 minutes and 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def split_file_and_save_parts():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PostgreSqlExample\") \\\n",
    "        .config(\"spark.jars\", \"postgresql.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:postgresql://postsql1:5432/finance\"\n",
    "    database_properties = {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "    \n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=\"fin_account_activity\", properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "split_file_and_save_parts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024d48ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 05:40:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/01 05:40:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "| id|              f_name|              l_name|       email_address|             country|          postalcode|           city_name|            province|          created_by|         created_dt|updated_by|updated_dt|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "|  1|Casey            ...|Diaz             ...|casey.diaz@irwina...|USA              ...|77099            ...|Houston          ...|TX               ...|root@%           ...|2023-07-10 14:08:54|      NULL|      NULL|\n",
      "|  2|Russell          ...|Cantrell         ...|russell.cantrell@...|USA              ...|77099            ...|Houston          ...|TX               ...|root@%           ...|2020-11-25 09:16:00|      NULL|      NULL|\n",
      "|  3|Jennifer         ...|Gutierrez        ...|jennifer.gutierre...|USA              ...|77095            ...|Houston          ...|TX               ...|root@%           ...|2023-05-31 19:14:12|      NULL|      NULL|\n",
      "|  4|Todd             ...|Patterson        ...|todd.patterson@ir...|USA              ...|77093            ...|Houston          ...|TX               ...|root@%           ...|2020-09-26 13:10:54|      NULL|      NULL|\n",
      "|  5|Steven           ...|Anderson         ...|steven.anderson@i...|USA              ...|77089            ...|Houston          ...|TX               ...|root@%           ...|2022-03-21 12:15:40|      NULL|      NULL|\n",
      "|  6|Denise           ...|Griffith         ...|denise.griffith@i...|USA              ...|77088            ...|Houston          ...|TX               ...|root@%           ...|2020-12-20 10:37:05|      NULL|      NULL|\n",
      "|  7|Mary             ...|Velez            ...|mary.velez@irwina...|USA              ...|77084            ...|Houston          ...|TX               ...|root@%           ...|2020-12-03 07:01:44|      NULL|      NULL|\n",
      "|  8|Christopher      ...|Carter           ...|christopher.carte...|USA              ...|77084            ...|Houston          ...|TX               ...|root@%           ...|2023-02-11 16:12:55|      NULL|      NULL|\n",
      "|  9|William          ...|Adams            ...|william.adams@irw...|USA              ...|77083            ...|Houston          ...|TX               ...|root@%           ...|2020-06-13 14:29:09|      NULL|      NULL|\n",
      "| 10|Tim              ...|Marks            ...|tim.marks@irwinan...|USA              ...|77083            ...|Houston          ...|TX               ...|root@%           ...|2019-12-14 11:11:40|      NULL|      NULL|\n",
      "| 11|Dawn             ...|Meadows          ...|dawn.meadows@irwi...|USA              ...|77082            ...|Houston          ...|TX               ...|root@%           ...|2023-01-26 13:48:30|      NULL|      NULL|\n",
      "| 12|Todd             ...|Oliver           ...|todd.oliver@irwin...|USA              ...|77082            ...|Houston          ...|TX               ...|root@%           ...|2019-12-29 00:11:00|      NULL|      NULL|\n",
      "| 13|Heather          ...|Stephens         ...|heather.stephens@...|USA              ...|77081            ...|Houston          ...|TX               ...|root@%           ...|2022-12-01 15:54:54|      NULL|      NULL|\n",
      "| 14|Kimberly         ...|Kim              ...|kimberly.kim@irwi...|USA              ...|77080            ...|Houston          ...|TX               ...|root@%           ...|2021-11-13 08:55:17|      NULL|      NULL|\n",
      "| 15|Connie           ...|Warren           ...|connie.warren@irw...|USA              ...|77077            ...|Houston          ...|TX               ...|root@%           ...|2022-05-31 00:11:29|      NULL|      NULL|\n",
      "| 16|Cindy            ...|Martin           ...|cindy.martin@irwi...|USA              ...|77073            ...|Houston          ...|TX               ...|root@%           ...|2021-08-25 20:02:01|      NULL|      NULL|\n",
      "| 17|Rodney           ...|Hansen           ...|rodney.hansen@irw...|USA              ...|77072            ...|Houston          ...|TX               ...|root@%           ...|2020-07-31 06:25:10|      NULL|      NULL|\n",
      "| 18|Chris            ...|Kim              ...|chris.kim@irwinan...|USA              ...|77070            ...|Houston          ...|TX               ...|root@%           ...|2019-04-28 17:31:58|      NULL|      NULL|\n",
      "| 19|Shannon          ...|Wilson           ...|shannon.wilson@ir...|USA              ...|77064            ...|Houston          ...|TX               ...|root@%           ...|2022-12-29 07:48:41|      NULL|      NULL|\n",
      "| 20|Amy              ...|Palmer           ...|amy.palmer@irwina...|USA              ...|77060            ...|Houston          ...|TX               ...|root@%           ...|2019-08-11 01:34:36|      NULL|      NULL|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 0 minutes and 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def test_database_extraction():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PostgreSqlExample\") \\\n",
    "        .config(\"spark.jars\", \"mysql-connector-j-8.2.0.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:mysql://mysql1:3306/customers\"\n",
    "    database_properties = {\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "    \n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=\"customer_info\", properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "test_database_extraction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe3fbe7-ab9c-43fe-a5ff-08bc2bfb33b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Time: 0 minutes and 7 seconds\n"
     ]
    }
   ],
   "source": [
    "#!tree\n",
    "# Stop the timer and calculate elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to minutes and seconds\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "\n",
    "print(f\"Overall Time: {minutes} minutes and {seconds} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
