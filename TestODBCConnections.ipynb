{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62292812-f394-447c-beaf-8b7d71d15a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from libraries.utility import Utility as mutil\n",
    "from libraries.db_base import db_base\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b405a0-a341-4fc9-bd1e-65f0e48a8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = mutil()\n",
    "# Start the timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b31a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 18 for SQL Server' : file not found (0) (SQLDriverConnect)\")\n",
      "Database : products, Connection Good: False\n"
     ]
    }
   ],
   "source": [
    "dbprod = db_base(\"products\")\n",
    "get_product_info_sql=dbprod.get_sql_query_from_query_key(\"get_product_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a1c6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT\\n    p.id,\\n    p.product_name,\\n    pt.product_type_desc as product_type\\nfrom products p\\nJOIN\\n    product_type pt\\n--on pt.id = p.id\\non pt.id = p.product_type_id\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c5c7c5-2d32-4683-b8b4-83cb17b15407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/29 19:48:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/29 19:48:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+--------------------+--------------------+----------+----------+-----------------+\n",
      "| id|        product_name|product_type_id|          created_by|          created_dt|updated_by|updated_dt|parent_product_id|\n",
      "+---+--------------------+---------------+--------------------+--------------------+----------+----------+-----------------+\n",
      "|  1|Donald's Date in ...|              5|sa               ...|2019-11-13 11:48:...|      NULL|      NULL|             NULL|\n",
      "|  2|Diana's Patriotic...|              8|sa               ...|2020-09-22 11:48:...|      NULL|      NULL|             NULL|\n",
      "|  3|Daniel's Crazy Co...|              5|sa               ...|2019-01-06 11:48:...|      NULL|      NULL|             NULL|\n",
      "|  4|Gregory's Date in...|             10|sa               ...|2020-04-11 11:48:...|      NULL|      NULL|             NULL|\n",
      "|  5|Jennifer's Wild Ride|              5|sa               ...|2019-11-17 11:48:...|      NULL|      NULL|             NULL|\n",
      "|  6|Patrick's Amazing...|              4|sa               ...|2019-08-10 11:48:...|      NULL|      NULL|             NULL|\n",
      "|  7|   April's Great War|             10|sa               ...|2019-06-07 11:48:...|      NULL|      NULL|             NULL|\n",
      "|  8|Emily's Crazy Comedy|              4|sa               ...|2019-09-26 11:48:...|      NULL|      NULL|             NULL|\n",
      "|  9|Rachel's Monster ...|              2|sa               ...|2019-01-26 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 10|Veronica's Patrio...|              7|sa               ...|2019-02-28 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 11|Robin's Patriotic...|              3|sa               ...|2019-10-23 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 12|John's Monster Ma...|              2|sa               ...|2020-08-15 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 13|Patrick's Crazy C...|             11|sa               ...|2019-09-17 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 14|Andrew's Patrioti...|             10|sa               ...|2019-05-03 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 15|Erik's Date in th...|              4|sa               ...|2020-03-18 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 16|Olivia's Big Romance|              2|sa               ...|2020-09-18 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 17| John's Spy Thriller|              5|sa               ...|2020-09-05 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 18|Jennifer's Date W...|              3|sa               ...|2020-07-05 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 19|Melissa's Spy Thr...|              2|sa               ...|2019-12-19 11:48:...|      NULL|      NULL|             NULL|\n",
      "| 20|Taylor's Trip in ...|              2|sa               ...|2019-04-03 11:48:...|      NULL|      NULL|             NULL|\n",
      "+---+--------------------+---------------+--------------------+--------------------+----------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 0 minutes and 4 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def split_file_and_save_parts():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MSSQLConnectionExample\") \\\n",
    "        .config(\"spark.jars\", \"mssql-jdbc-12.4.2.jre11.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:sqlserver://mssql1:1433;databaseName=products\"\n",
    "    database_properties = {\n",
    "        \"user\": \"sa\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "\n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=\"products\", properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "split_file_and_save_parts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dced2979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 19:48:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+----------+---------+-------+----------------+---------------------+---------------------+----------+----------+----------+----------+\n",
      "| id|product_id|customer_id|account_id|post_date|amt_usd|geo_geography_id|fin_distro_channel_id|fin_distro_partner_id|created_by|created_dt|updated_by|updated_dt|\n",
      "+---+----------+-----------+----------+---------+-------+----------------+---------------------+---------------------+----------+----------+----------+----------+\n",
      "+---+----------+-----------+----------+---------+-------+----------------+---------------------+---------------------+----------+----------+----------+----------+\n",
      "\n",
      "Time elapsed: 0 minutes and 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def split_file_and_save_parts():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PostgreSqlExample\") \\\n",
    "        .config(\"spark.jars\", \"postgresql.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:postgresql://postsql1:5432/finance\"\n",
    "    database_properties = {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "    \n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=\"fin_account_activity\", properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "split_file_and_save_parts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024d48ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 19:48:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "| id|              f_name|              l_name|       email_address|             country|          postalcode|           city_name|            province|          created_by|         created_dt|updated_by|updated_dt|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "|  1|Patrick          ...|Johnson          ...|patrick.johnson@i...|USA              ...|77099            ...|Houston          ...|TX               ...|root@%           ...|2019-04-01 21:59:33|      NULL|      NULL|\n",
      "|  2|Manuel           ...|Boyer            ...|manuel.boyer@irwi...|USA              ...|77099            ...|Houston          ...|TX               ...|root@%           ...|2020-03-11 02:09:04|      NULL|      NULL|\n",
      "|  3|Christina        ...|Hinton           ...|christina.hinton@...|USA              ...|77095            ...|Houston          ...|TX               ...|root@%           ...|2022-03-02 15:45:20|      NULL|      NULL|\n",
      "|  4|William          ...|Blankenship      ...|william.blankensh...|USA              ...|77093            ...|Houston          ...|TX               ...|root@%           ...|2019-06-01 19:47:31|      NULL|      NULL|\n",
      "|  5|Donna            ...|Gonzalez         ...|donna.gonzalez@ir...|USA              ...|77089            ...|Houston          ...|TX               ...|root@%           ...|2019-08-15 19:40:39|      NULL|      NULL|\n",
      "|  6|Steven           ...|Yang             ...|steven.yang@irwin...|USA              ...|77088            ...|Houston          ...|TX               ...|root@%           ...|2022-08-10 20:40:41|      NULL|      NULL|\n",
      "|  7|Kevin            ...|Sanders          ...|kevin.sanders@irw...|USA              ...|77084            ...|Houston          ...|TX               ...|root@%           ...|2019-08-05 03:18:11|      NULL|      NULL|\n",
      "|  8|Joseph           ...|Dennis           ...|joseph.dennis@irw...|USA              ...|77084            ...|Houston          ...|TX               ...|root@%           ...|2019-05-27 17:36:59|      NULL|      NULL|\n",
      "|  9|Eric             ...|Contreras        ...|eric.contreras@ir...|USA              ...|77083            ...|Houston          ...|TX               ...|root@%           ...|2020-11-22 19:34:50|      NULL|      NULL|\n",
      "| 10|Cheryl           ...|Wilson           ...|cheryl.wilson@irw...|USA              ...|77083            ...|Houston          ...|TX               ...|root@%           ...|2020-01-26 20:14:13|      NULL|      NULL|\n",
      "| 11|Angela           ...|Jones            ...|angela.jones@irwi...|USA              ...|77082            ...|Houston          ...|TX               ...|root@%           ...|2022-07-05 04:42:00|      NULL|      NULL|\n",
      "| 12|Susan            ...|Carroll          ...|susan.carroll@irw...|USA              ...|77082            ...|Houston          ...|TX               ...|root@%           ...|2019-11-11 16:47:09|      NULL|      NULL|\n",
      "| 13|Amber            ...|Green            ...|amber.green@irwin...|USA              ...|77081            ...|Houston          ...|TX               ...|root@%           ...|2023-05-31 00:19:27|      NULL|      NULL|\n",
      "| 14|Paige            ...|Dickerson        ...|paige.dickerson@i...|USA              ...|77080            ...|Houston          ...|TX               ...|root@%           ...|2022-05-21 14:06:13|      NULL|      NULL|\n",
      "| 15|Michael          ...|Martinez         ...|michael.martinez@...|USA              ...|77077            ...|Houston          ...|TX               ...|root@%           ...|2019-08-31 17:22:10|      NULL|      NULL|\n",
      "| 16|David            ...|Mercer           ...|david.mercer@irwi...|USA              ...|77073            ...|Houston          ...|TX               ...|root@%           ...|2021-09-09 07:08:40|      NULL|      NULL|\n",
      "| 17|Melissa          ...|Hoffman          ...|melissa.hoffman@i...|USA              ...|77072            ...|Houston          ...|TX               ...|root@%           ...|2023-07-18 07:50:29|      NULL|      NULL|\n",
      "| 18|Megan            ...|Hunter           ...|megan.hunter@irwi...|USA              ...|77070            ...|Houston          ...|TX               ...|root@%           ...|2021-06-16 13:47:00|      NULL|      NULL|\n",
      "| 19|Deborah          ...|Dixon            ...|deborah.dixon@irw...|USA              ...|77064            ...|Houston          ...|TX               ...|root@%           ...|2019-08-23 01:32:12|      NULL|      NULL|\n",
      "| 20|Timothy          ...|May              ...|timothy.may@irwin...|USA              ...|77060            ...|Houston          ...|TX               ...|root@%           ...|2021-03-22 00:07:40|      NULL|      NULL|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 0 minutes and 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def test_database_extraction():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PostgreSqlExample\") \\\n",
    "        .config(\"spark.jars\", \"mysql-connector-j-8.2.0.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:mysql://mysql1:3306/customers\"\n",
    "    database_properties = {\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "    \n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=\"customer_info\", properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "test_database_extraction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe3fbe7-ab9c-43fe-a5ff-08bc2bfb33b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Time: 0 minutes and 8 seconds\n"
     ]
    }
   ],
   "source": [
    "#!tree\n",
    "# Stop the timer and calculate elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to minutes and seconds\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "\n",
    "print(f\"Overall Time: {minutes} minutes and {seconds} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
